{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment - 5\n",
    "#### execute the code in python-3 along with weights_final.h5py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avni/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(total_sample, input_data, numbers):\n",
    "    y = np.zeros((total_sample, numbers))\n",
    "    for i in range(total_sample):\n",
    "        position = input_data[i][0]\n",
    "        y[i][position] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 4, 3, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y = np.load('ex5_train_y.npy')\n",
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.reshape(data_y, (data_y.shape[0],1))\n",
    "y = one_hot_encoding(1020, Y, 6)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x = np.load('ex5_train_x.npy')\n",
    "X_norm = data_x/255 - 0.5 #normalizing data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(m, A2, y):\n",
    "    new_cost = (-1/m) * np.sum(np.multiply(y, np.log(A2)) + np.multiply(1 - y, np.log(1 - A2)))\n",
    "    return new_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):     \n",
    "    return 1/ (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return (Z * (Z > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "      \n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "   \n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "  \n",
    "    Z = np.sum(s)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # dimensions from A_prev's shape \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # dimensions from W's shape \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    # information from \"hparameters\" \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "\n",
    "    \n",
    "    # dimensions of the CONV output \n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Creating A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):   \n",
    "#         print \"i= \", i                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:]                   # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                   \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = h*stride + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end =w*stride + f\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "#                     print Z.shape\n",
    "                    # activation of convolution layer using RELU\n",
    "                    A[i, h, w, c] = relu(Z[i, h, w, c])\n",
    "                    \n",
    "    \n",
    "    # Saving information in \"cache\" for the back propagation\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode ):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "  \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "\n",
    "    A = np.zeros((m, n_H, n_W, n_C))  \n",
    "    P = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "   \n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                 \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = h * stride + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = w * stride + f\n",
    "                    \n",
    "                   \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                   \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"avg\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # storing the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    \n",
    "    return A , cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(W1,W2,X,b1,b2):\n",
    "    \n",
    "    Z1 = np.add(np.dot(W1, X.T) , b1) \n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.add(np.dot(W2,A1), b2)\n",
    "    A2 = sigmoid(Z2)   \n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3, 8)\n",
      "(1, 1, 1, 8)\n",
      "(4, 4, 8, 16)\n",
      "(1, 1, 1, 16)\n",
      "(108, 1296)\n",
      "(108, 1)\n",
      "(6, 108)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "# loading the latest weights from weight_final.hdf5 \n",
    "weights_file = h5py.File(\"weights_final.hdf5\", \"r\")\n",
    "W_conv1 = np.array(weights_file[\"W1\"])\n",
    "b_conv1 = np.array(weights_file[\"b1\"])\n",
    "W_conv2 = np.array(weights_file[\"W2\"])\n",
    "b_conv2 = np.array(weights_file[\"b2\"])\n",
    "W1 = np.array(weights_file[\"W3\"])\n",
    "b1 = np.array(weights_file[\"b3\"])\n",
    "W2 = np.array(weights_file[\"W4\"])\n",
    "b2 = np.array(weights_file[\"b4\"])\n",
    "print (W_conv1.shape)\n",
    "print (b_conv1.shape)\n",
    "print (W_conv2.shape)\n",
    "print (b_conv2.shape)\n",
    "print (W1.shape)\n",
    "print (b1.shape)\n",
    "print (W2.shape)\n",
    "print (b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_prime(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    " \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    " \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "   \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                 \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "               \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        if(pad == 0):\n",
    "            dA_prev [i,:,:,:] = da_prev_pad [:,:,:]\n",
    "            \n",
    "        elif(pad > 0):\n",
    "            \n",
    "            dA_prev [i,:,:,:] = da_prev_pad [pad :- pad ,pad : -pad,:]\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\" \n",
    "    mask = x == np.max(x)    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "  \n",
    "    average = dz / (n_H * n_W)\n",
    " \n",
    "    a = np.ones(shape) * average\n",
    "  \n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "   \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    " \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                  \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                \n",
    "                    if mode == \"max\":\n",
    "               \n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                      \n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                       \n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"avg\":\n",
    "                      \n",
    "                        da = dA[i, h, w, c]\n",
    "                 \n",
    "                        shape = (f, f)\n",
    "                      \n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(W1, b1, db1, dW1, W2, b2, db2, dW2, alpha):   \n",
    "    W1_new = W1 - (alpha * dW1)\n",
    "    b1_new = b1 - (alpha * db1)\n",
    "    W2_new = W2 - (alpha * dW2)\n",
    "    b2_new = b2 - (alpha * db2)\n",
    "    return W1_new, b1_new, W2_new, b2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation_neural_network(m, A2,A1,W1, W2, X, y):\n",
    "    dZ2 = A2 - y.T  # shape (6,1020)\n",
    "    \n",
    "    dW2 = (1/m)*(np.dot(dZ2 , A1.T))   # shape (6, 108)\n",
    "    db2 = (1/m)*(np.sum(dZ2, axis = 1, keepdims = True)) # shape (6,1)\n",
    "        \n",
    "    dA1= np.dot(dZ2.T, W2)   # shape (108,1020)\n",
    "    dZ1 = np.multiply(dA1.T, np.multiply(A1, (1- A1))) # shape (108, 1020)\n",
    "    dW1 = (1/m)*(np.dot(dZ1, X))    # shape (108, 1296)\n",
    "    db1 = (1/m)*(np.sum(dZ1, axis = 1, keepdims = True))  # shape (108,1)\n",
    "    \n",
    "    \n",
    "    dA2 = np.dot(dZ1.T, W1) #shape (1020, 1296)\n",
    "    return dZ1, dZ2, dW1, dW2, db1, db2, dA2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (1020, 32, 32, 8)\n",
      "P1 (1020, 28, 28, 8)\n",
      "Z2 (1020, 13, 13, 16)\n",
      "P2.shape (1020, 9, 9, 16)\n",
      "F3.shape (1020, 1296)\n",
      "Cost = 2.703270364601317\n",
      "dZ1.shape = (108, 1020)\n",
      "dZ2.shape = (6, 1020)\n",
      "dW1.shape = (108, 1296)\n",
      "dW2.shape = (6, 108)\n",
      "db1.shape = (108, 1)\n",
      "db2.shape = (6, 1)\n",
      "dA2.shape = (1020, 9, 9, 16)\n",
      "dA_conv2.shape = (1020, 13, 13, 16)\n",
      "dZ_conv2.shape (1020, 13, 13, 16)\n",
      "dP1 shape = (1020, 28, 28, 8)\n",
      "dW_conv1 shape = (4, 4, 8, 16)\n",
      "db_conv1 shape = (1, 1, 1, 16)\n",
      "dA_conv1  (1020, 32, 32, 8)\n",
      "dZ_conv1 shape  (1020, 32, 32, 8)\n",
      "dX shape = (1020, 64, 64, 3)\n",
      "dW_conv1 shape = (4, 4, 3, 8)\n",
      "db_conv1 shape = (1, 1, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "iters = 0\n",
    "alpha = 1\n",
    "for iters in range(epochs):\n",
    "    hparameters_conv1 = {\"pad\": 1, \"stride\":2 }\n",
    "    Z1, A1 , cache_conv1 = conv_forward(X_norm, W_conv1, b_conv1, hparameters_conv1)\n",
    "    print (\"Z1\", Z1.shape)\n",
    "\n",
    "    hparameters_pool1 = {\"f\": 5 , \"stride\": 1 }\n",
    "\n",
    "    P1, cache_pool1 = pool_forward(A1, hparameters_pool1, mode = \"max\")\n",
    "\n",
    "    print (\"P1\", P1.shape)\n",
    "\n",
    "    hparameters_conv2 = {\"pad\": 0, \"stride\":2}\n",
    "\n",
    "    Z2, A2 , cache_conv2 = conv_forward(P1, W_conv2, b_conv2, hparameters_conv2)\n",
    "\n",
    "    print (\"Z2\", Z2.shape)\n",
    "\n",
    "    hparameters_pool2 = {\"f\": 5 , \"stride\": 1 }\n",
    "\n",
    "    P2, cache_pool2 = pool_forward(A2, hparameters_pool2, mode = \"avg\")\n",
    "\n",
    "    print (\"P2.shape\", P2.shape)\n",
    "    F3 = []\n",
    "    for i in range (P2.shape[0]):\n",
    "        mat = P2[i]\n",
    "        mat_i = mat.flatten()\n",
    "        F3.append(mat_i)\n",
    "    F3= np.asarray(F3) \n",
    "    print (\"F3.shape\", F3.shape)\n",
    "    m = y.shape[0]\n",
    "    Z1, A1, Z2, A2 = fully_connected(W1,W2,F3,b1,b2)   \n",
    "    \"Cost calculation\"\n",
    "    A2T= A2.T\n",
    "    cost = (-1/m) * np.sum(np.multiply(y, np.log(A2T)) + np.multiply(1 - y, np.log(1 - A2T)))\n",
    "    print (\"Cost =\", cost)\n",
    "    dZ1, dZ2, dW1, dW2, db1, db2, dA2 = backward_propagation_neural_network(m, A2,A1,W1, W2, F3, y)\n",
    "    print (\"dZ1.shape =\", dZ1.shape)\n",
    "    print (\"dZ2.shape =\", dZ2.shape)\n",
    "    print (\"dW1.shape =\", dW1.shape)\n",
    "    print (\"dW2.shape =\", dW2.shape)\n",
    "    print (\"db1.shape =\", db1.shape)\n",
    "    print (\"db2.shape =\", db2.shape)\n",
    "    \n",
    "    W1_new, b1_new, W2_new, b2_new= gradient_descent(W1, b1, db1, dW1, W2, b2, db2, dW2, alpha)\n",
    "    dA2= np.reshape(dA2, (dA2.shape[0],9,9,16))\n",
    "    print (\"dA2.shape =\", dA2.shape)\n",
    "    dA_conv2 = pool_backward(dA2, cache_pool2, mode= \"avg\")\n",
    "    print (\"dA_conv2.shape =\", dA_conv2.shape)\n",
    "    dZ_conv2 = relu_prime(dA_conv2)\n",
    "    print (\"dZ_conv2.shape\", dZ_conv2.shape)\n",
    "    dP1, dW_conv2, db_conv2 = conv_backward(dZ_conv2, cache_conv2)\n",
    "    print (\"dP1 shape =\", dP1.shape)\n",
    "    print (\"dW_conv1 shape =\", dW_conv2.shape)\n",
    "    print (\"db_conv1 shape =\", db_conv2.shape)\n",
    "    dA_conv1 = pool_backward(dP1, cache_pool1, mode= \"max\")\n",
    "    print(\"dA_conv1 \",dA_conv1.shape)\n",
    "    dZ_conv1 = relu_prime(dA_conv1)\n",
    "    print(\"dZ_conv1 shape \",dZ_conv1.shape)\n",
    "    dX, dW_conv1, db_conv1 = conv_backward(dZ_conv1, cache_conv1)\n",
    "    print(\"dX shape =\",dX.shape)\n",
    "    print(\"dW_conv1 shape =\",dW_conv1.shape)\n",
    "    print(\"db_conv1 shape =\",db_conv1.shape)\n",
    "    \n",
    "    W_conv1,W_conv2,b_conv1,b_conv2 = gradient_descent(W_conv1, b_conv1, db_conv1, dW_conv1, W_conv2, b_conv2, db_conv2, dW_conv2, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost = 2.703270364601317\n"
     ]
    }
   ],
   "source": [
    "print (\"Cost =\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 72.96729635398682\n"
     ]
    }
   ],
   "source": [
    "Score = (10-cost)*10\n",
    "print (\"Score =\", Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost = 2.703270364601317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
